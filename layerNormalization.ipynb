{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38248829",
   "metadata": {},
   "source": [
    "# Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9e9a86",
   "metadata": {},
   "source": [
    "## Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf79b42f",
   "metadata": {},
   "source": [
    "之前有讲过[Batch Normalization](https://github.com/MorvanLi/deepLearningTutorial/blob/main/batchNormalization.ipynb)的原理，今天来简单讲讲`Layer Normalization`。Layer Normalization是针对自然语言处理领域提出的，例如像RNN循环神经网络。为什么不使用直接BN呢，因为在RNN这类时序网络中，时序的长度并不是一个定值（网络深度不一定相同），比如每句话的长短都不一定相同，所有很难去使用BN，所以作者提出了Layer Normalization（注意，在图像处理领域中BN比LN是更有效的，但现在很多人将自然语言领域的模型用来处理图像，比如Vision Transformer，此时还是会涉及到LN）。直接看下Pytorch官方给的关于LayerNorm的简单介绍。只看公式的话感觉和BN没什么区别，都是减去均值$E(x)$，除于方差$\\sqrt{Var(x)+\\varepsilon } $其中$\\varepsilon$是一个非常小的量（默认为$10^{-5}$），是为了防止分母为零。同样也有两个可训练的参数。不同的是，BN是对一个batch数据的每个channel进行Norm处理，但LN是对单个数据的指定维度进行Norm处理与batch无关（后面有示例）。而且在BN中训练时是需要累计$moving_{mean}$和$moving_{var}$两个变量的（所以在BN中需要四个参数$moving_{mean}$,$moving_{var}$，$\\beta$和$\\gamma$，但LN不需要累计只有$\\beta$，$\\gamma$ 。\n",
    "\n",
    "在Pytorch的LayerNorm类中有个**normalized_shape**参数，可以指定你要Norm的维度（注意，函数说明中the last certain number of dimensions，指定的维度必须是从最后一维开始）。比如我们的数据的shape是[4, 2, 3]，那么normalized_shape可以是[3]（最后一维上进行Norm处理），也可以是[2, 3]（Norm最后两个维度），也可以是[4, 2, 3]（对整个维度进行Norm），但不能是[2]或者[4, 2]，否则会报以下错误（以normalized_shape=[2]为例）："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f67033e",
   "metadata": {},
   "source": [
    "## Pytorch 实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0586e311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.6935, 0.5683, 0.9867],\n",
      "         [0.4870, 0.6400, 0.5081]],\n",
      "\n",
      "        [[0.7813, 0.3420, 0.9768],\n",
      "         [0.6240, 0.1320, 0.0312]],\n",
      "\n",
      "        [[0.9584, 0.8086, 0.1088],\n",
      "         [0.7080, 0.6783, 0.6243]],\n",
      "\n",
      "        [[0.9419, 0.8908, 0.9590],\n",
      "         [0.3025, 0.1646, 0.2289]]])\n",
      "t1:\n",
      " tensor([[[-0.3193, -1.0333,  1.3526],\n",
      "         [-0.8564,  1.4012, -0.5448]],\n",
      "\n",
      "        [[ 0.3062, -1.3487,  1.0425],\n",
      "         [ 1.3961, -0.5034, -0.8927]],\n",
      "\n",
      "        [[ 0.8997,  0.4951, -1.3947],\n",
      "         [ 1.0856,  0.2341, -1.3198]],\n",
      "\n",
      "        [[ 0.3883, -1.3644,  0.9761],\n",
      "         [ 1.2493, -1.1945, -0.0549]]], grad_fn=<NativeLayerNormBackward>)\n",
      "t2:\n",
      " tensor([[[-0.3193, -1.0333,  1.3526],\n",
      "         [-0.8564,  1.4012, -0.5448]],\n",
      "\n",
      "        [[ 0.3062, -1.3487,  1.0425],\n",
      "         [ 1.3961, -0.5034, -0.8927]],\n",
      "\n",
      "        [[ 0.8997,  0.4951, -1.3947],\n",
      "         [ 1.0856,  0.2341, -1.3198]],\n",
      "\n",
      "        [[ 0.3883, -1.3643,  0.9760],\n",
      "         [ 1.2493, -1.1945, -0.0549]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def layer_norm_process(feature: torch.Tensor, beta=0., gamma=1., eps=1e-5):\n",
    "    var_mean = torch.var_mean(feature, dim=-1, unbiased=False)\n",
    "    # 均值   -------> [[4, 2]]\n",
    "    mean = var_mean[1]   \n",
    "    # 方差-------> [[4, 2]]\n",
    "    var = var_mean[0]\n",
    "\n",
    "    # layer norm process  mean[..., None]== [[[4, 2, 1]]]在最后添加一个维度\n",
    "    feature = (feature - mean[..., None]) / torch.sqrt(var[..., None] + eps)\n",
    "    feature = feature * gamma + beta\n",
    "\n",
    "    return feature\n",
    "\n",
    "\n",
    "def main():\n",
    "    t = torch.rand(4, 2, 3)\n",
    "    print(t)\n",
    "    # 仅在最后一个维度上做norm处理\n",
    "    norm = nn.LayerNorm(normalized_shape=t.shape[-1], eps=1e-5)\n",
    "    # 官方layer norm处理\n",
    "    t1 = norm(t)\n",
    "    # 自己实现的layer norm处理\n",
    "    t2 = layer_norm_process(t, eps=1e-5)\n",
    "    print(\"t1:\\n\", t1)\n",
    "    print(\"t2:\\n\", t2)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8fab47",
   "metadata": {},
   "source": [
    "## 关于均值tensor通道"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc38904",
   "metadata": {},
   "source": [
    "torch.mean()是计算整个tensor的均值，tensor.mean(dim)是计算指定维度的均值，例如tensor.mean(0)是计算第一个维度的均值，**`并随之将该维度压缩`**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe8d708b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始的数据为:\n",
      " tensor([[[0.4963, 0.7682, 0.0885],\n",
      "         [0.1320, 0.3074, 0.6341],\n",
      "         [0.4901, 0.8964, 0.4556]],\n",
      "\n",
      "        [[0.6323, 0.3489, 0.4017],\n",
      "         [0.0223, 0.1689, 0.2939],\n",
      "         [0.5185, 0.6977, 0.8000]],\n",
      "\n",
      "        [[0.1610, 0.2823, 0.6816],\n",
      "         [0.9152, 0.3971, 0.8742],\n",
      "         [0.4194, 0.5529, 0.9527]],\n",
      "\n",
      "        [[0.0362, 0.1852, 0.3734],\n",
      "         [0.3051, 0.9320, 0.1759],\n",
      "         [0.2698, 0.1507, 0.0317]]])\n",
      "\n",
      "tensor.mean()的值为: \n",
      "0.44025862216949463\n",
      "\n",
      "tensor.mean(0)的值为:　\n",
      "tensor([[0.3314, 0.3962, 0.3863],\n",
      "        [0.3437, 0.4513, 0.4945],\n",
      "        [0.4245, 0.5744, 0.5600]]) \n",
      " 将第一个维度压缩:torch.Size([3, 3])\n",
      "\n",
      "tensor.mean(1)的值为:　\n",
      "tensor([[0.3728, 0.6574, 0.3927],\n",
      "        [0.3911, 0.4051, 0.4985],\n",
      "        [0.4985, 0.4108, 0.8362],\n",
      "        [0.2037, 0.4226, 0.1937]]) \n",
      " 将第二个维度压缩:torch.Size([4, 3])\n",
      "\n",
      "tensor.mean(2)的值为:　\n",
      "tensor([[0.4510, 0.3578, 0.6141],\n",
      "        [0.4610, 0.1617, 0.6721],\n",
      "        [0.3750, 0.7288, 0.6417],\n",
      "        [0.1983, 0.4710, 0.1507]]) \n",
      " 将第三个维度压缩:torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0) \n",
    "tensor = torch.rand(4,3,3)\n",
    "print(f\"原始的数据为:\\n {tensor}\")\n",
    "print(\"\")\n",
    "print(f'tensor.mean()的值为: \\n{tensor.mean()}')\n",
    "print(\"\")\n",
    "print(f'tensor.mean(0)的值为:　\\n{tensor.mean(0)} \\n 将第一个维度压缩:{(tensor.mean(0)).shape}')\n",
    "print(\"\")\n",
    "print(f'tensor.mean(1)的值为:　\\n{tensor.mean(1)} \\n 将第二个维度压缩:{(tensor.mean(1)).shape}')\n",
    "print(\"\")\n",
    "print(f'tensor.mean(2)的值为:　\\n{tensor.mean(2)} \\n 将第三个维度压缩:{(tensor.mean(2)).shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7aad76",
   "metadata": {},
   "source": [
    "为什么答案是这样的呢？\n",
    "\n",
    "tensor包含了4个$3*3$的二维数组，tensor.mean(0)就是计算这4个$3*3$的均值。\n",
    "$$ (0.4963+0.0.23+0.1610+0.0362)/4=0.3314 $$\n",
    "$$ (0.7682+0.3489+0.2823+0.1852)/4=0.3962 $$\n",
    "$$ (0.0885+0.4017+0.6816+0.3734)/4=0.3863 $$\n",
    "\n",
    "tensor.mean(1):实质就是求每列的均值\n",
    "$$(0.4963+0.1320+0.4901)/3=0.3728$$\n",
    "$$(0.7682+0.3074+0.8964)/3=0.6573$$\n",
    "$$(0.0885+0.6341+0.4556)/3=0.3927$$\n",
    "\n",
    "tensor.mean(2):实质就是求每行的均值\n",
    "$$(0.4963+0.7682+0.0885)/3=0.4510$$\n",
    "$$(0.1320+0.3074+0.6341)/3=0.3578$$\n",
    "$$(0.4901+0.8964+0.4556)/3=0.6141$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266f9faf",
   "metadata": {},
   "source": [
    "再举一个例子：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "35a9f415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始的数据为:\n",
      " tensor([[[[0.4963, 0.7682, 0.0885],\n",
      "          [0.1320, 0.3074, 0.6341],\n",
      "          [0.4901, 0.8964, 0.4556]],\n",
      "\n",
      "         [[0.6323, 0.3489, 0.4017],\n",
      "          [0.0223, 0.1689, 0.2939],\n",
      "          [0.5185, 0.6977, 0.8000]]],\n",
      "\n",
      "\n",
      "        [[[0.1610, 0.2823, 0.6816],\n",
      "          [0.9152, 0.3971, 0.8742],\n",
      "          [0.4194, 0.5529, 0.9527]],\n",
      "\n",
      "         [[0.0362, 0.1852, 0.3734],\n",
      "          [0.3051, 0.9320, 0.1759],\n",
      "          [0.2698, 0.1507, 0.0317]]],\n",
      "\n",
      "\n",
      "        [[[0.2081, 0.9298, 0.7231],\n",
      "          [0.7423, 0.5263, 0.2437],\n",
      "          [0.5846, 0.0332, 0.1387]],\n",
      "\n",
      "         [[0.2422, 0.8155, 0.7932],\n",
      "          [0.2783, 0.4820, 0.8198],\n",
      "          [0.9971, 0.6984, 0.5675]]],\n",
      "\n",
      "\n",
      "        [[[0.8352, 0.2056, 0.5932],\n",
      "          [0.1123, 0.1535, 0.2417],\n",
      "          [0.7262, 0.7011, 0.2038]],\n",
      "\n",
      "         [[0.6511, 0.7745, 0.4369],\n",
      "          [0.5191, 0.6159, 0.8102],\n",
      "          [0.9801, 0.1147, 0.3168]]]])\n",
      "\n",
      "tensor.mean()的值为: \n",
      "0.4814554750919342\n",
      "\n",
      "tensor.mean(0)的值为:　\n",
      "tensor([[[0.4252, 0.5465, 0.5216],\n",
      "         [0.4755, 0.3461, 0.4984],\n",
      "         [0.5551, 0.5459, 0.4377]],\n",
      "\n",
      "        [[0.3904, 0.5310, 0.5013],\n",
      "         [0.2812, 0.5497, 0.5249],\n",
      "         [0.6914, 0.4154, 0.4290]]]) \n",
      " 将第一个维度压缩:torch.Size([2, 3, 3])\n",
      "\n",
      "tensor.mean(1)的值为:　\n",
      "tensor([[[0.5643, 0.5586, 0.2451],\n",
      "         [0.0772, 0.2381, 0.4640],\n",
      "         [0.5043, 0.7971, 0.6278]],\n",
      "\n",
      "        [[0.0986, 0.2337, 0.5275],\n",
      "         [0.6101, 0.6646, 0.5250],\n",
      "         [0.3446, 0.3518, 0.4922]],\n",
      "\n",
      "        [[0.2252, 0.8726, 0.7581],\n",
      "         [0.5103, 0.5041, 0.5317],\n",
      "         [0.7908, 0.3658, 0.3531]],\n",
      "\n",
      "        [[0.7431, 0.4900, 0.5150],\n",
      "         [0.3157, 0.3847, 0.5259],\n",
      "         [0.8532, 0.4079, 0.2603]]]) \n",
      " 将第二个维度压缩:torch.Size([4, 3, 3])\n",
      "\n",
      "tensor.mean(2)的值为:　\n",
      "tensor([[[0.3728, 0.6574, 0.3927],\n",
      "         [0.3911, 0.4051, 0.4985]],\n",
      "\n",
      "        [[0.4985, 0.4108, 0.8362],\n",
      "         [0.2037, 0.4226, 0.1937]],\n",
      "\n",
      "        [[0.5117, 0.4964, 0.3685],\n",
      "         [0.5059, 0.6653, 0.7268]],\n",
      "\n",
      "        [[0.5579, 0.3534, 0.3462],\n",
      "         [0.7167, 0.5017, 0.5213]]]) \n",
      " 将第三个维度压缩:torch.Size([4, 2, 3])\n",
      "\n",
      "tensor.mean(3)的值为:　\n",
      "tensor([[[0.4510, 0.3578, 0.6141],\n",
      "         [0.4610, 0.1617, 0.6721]],\n",
      "\n",
      "        [[0.3750, 0.7288, 0.6417],\n",
      "         [0.1983, 0.4710, 0.1507]],\n",
      "\n",
      "        [[0.6203, 0.5041, 0.2522],\n",
      "         [0.6170, 0.5267, 0.7544]],\n",
      "\n",
      "        [[0.5447, 0.1692, 0.5437],\n",
      "         [0.6208, 0.6484, 0.4705]]]) \n",
      " 将第三个维度压缩:torch.Size([4, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0) \n",
    "tensor1 = torch.rand(4,2,3,3)\n",
    "print(f\"原始的数据为:\\n {tensor1}\")\n",
    "print(\"\")\n",
    "print(f'tensor.mean()的值为: \\n{tensor1.mean()}')\n",
    "print(\"\")\n",
    "print(f'tensor.mean(0)的值为:　\\n{tensor1.mean(0)} \\n 将第一个维度压缩:{(tensor1.mean(0)).shape}')\n",
    "print(\"\")\n",
    "print(f'tensor.mean(1)的值为:　\\n{tensor1.mean(1)} \\n 将第二个维度压缩:{(tensor1.mean(1)).shape}')\n",
    "print(\"\")\n",
    "print(f'tensor.mean(2)的值为:　\\n{tensor1.mean(2)} \\n 将第三个维度压缩:{(tensor1.mean(2)).shape}')\n",
    "print(\"\")\n",
    "print(f'tensor.mean(3)的值为:　\\n{tensor1.mean(3)} \\n 将第三个维度压缩:{(tensor1.mean(3)).shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de00f60",
   "metadata": {},
   "source": [
    "tensor.mean(0)就是计算这tensor第0个维度的均值:\n",
    "$$0.4963+0.1610+0.2081+0.8352=0.4252$$\n",
    "\n",
    "tensor.mean(1)就是计算这tensor第1个维度的均值:\n",
    "$$(0.4963+0.6323)/2=0.5643$$\n",
    "\n",
    "tensor.mean(2)就是计算这tensor第2个维度的均值:\n",
    "$$(0.4963+0.1320+0.4901)/3=0.3728$$\n",
    "\n",
    "tensor.mean(3)就是计算这tensor第3个维度的均值:\n",
    "$$(0.4963+0.7682+0.0885)/3=0.4510$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27f7230",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
